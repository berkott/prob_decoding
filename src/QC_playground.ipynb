{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.linear_model import Ridge, ARDRegression\n",
    "from models.reduced_rank_model import ReducedRankModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURON_CLUSTERS = 3\n",
    "RANK = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./raw_data/full_data/names_and_shapes.txt') as file:\n",
    "    lines = []\n",
    "    for line in file:\n",
    "        sep = line.find(',')\n",
    "        name = line[:sep]\n",
    "        lines.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "Y_list = []\n",
    "\n",
    "for i in range(len(lines) // 2):\n",
    "    X_file = './raw_data/full_data/' + lines[i]\n",
    "    X_i = np.load(X_file)\n",
    "    X_list.append(X_i)\n",
    "    Y_file = './raw_data/full_data/' + lines[i + len(lines) // 2]\n",
    "    Y_i = np.load(Y_file)\n",
    "    Y_list.append(Y_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_sessions = len(X_list)\n",
    "num_sessions = 5\n",
    "time_bins = X_list[0].shape[2]\n",
    "train_split = 0.8\n",
    "train_iters = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [X[:int(train_split * X.shape[0])] for X in X_list]\n",
    "X_test = [X[int(train_split * X.shape[0]):] for X in X_list]\n",
    "Y_train = [Y[:int(train_split * Y.shape[0])] for Y in Y_list]\n",
    "Y_test = [Y[int(train_split * Y.shape[0]):] for Y in Y_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set neuron cluster thresholds based on average spikes per session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average spikes per neuron per session\n",
    "avg_spikes_lst = []\n",
    "for i in range(len(X_train)):\n",
    "    avg_spikes_lst.append(np.mean(np.sum(X_train[i], axis=2), axis=0))\n",
    "\n",
    "avg_spikes = np.concatenate(avg_spikes_lst, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of avg_spikes, ranked in descending order\n",
    "avg_spikes = np.sort(avg_spikes)[::-1]\n",
    "fig = go.Figure(data=[go.Bar(x=np.arange(len(avg_spikes)), y=avg_spikes)])\n",
    "fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)', marker_line_width=1.5,\n",
    "                  opacity=0.6)\n",
    "fig.update_layout(title='Average Spikes per Neuron per Session', xaxis_title='Neuron', yaxis_title='Average Spikes')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of avg spikes thresholds for neuron clusters\n",
    "def get_thresholds(avg_spikes, num_clusters, splits=None):\n",
    "    thresholds = []\n",
    "    if splits is not None and len(splits) == num_clusters - 1:\n",
    "        thresholds.append(avg_spikes[0])\n",
    "        for i in range(num_clusters - 1):\n",
    "            thresholds.append(avg_spikes[int(splits[i] * len(avg_spikes))])\n",
    "    else:\n",
    "        for i in range(num_clusters):\n",
    "            thresholds.append(avg_spikes[int(i * len(avg_spikes) / num_clusters)])\n",
    "    thresholds.append(avg_spikes[-1])\n",
    "    thresholds[0] += 1\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster neurons in training data per thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster neurons based on avg spikes per neuron\n",
    "def cluster_neurons(X_old, thresholds, num_clusters):\n",
    "    X_new = []\n",
    "\n",
    "    for X in X_old:\n",
    "        avg_spikes_per_neuron = np.mean(np.sum(X, axis=2), axis=0)\n",
    "        neuron_clusters = []\n",
    "        for j in range(num_clusters):\n",
    "            neuron_clusters.append(np.where((avg_spikes_per_neuron < thresholds[j]) &\n",
    "                                            (avg_spikes_per_neuron >= thresholds[j+1]))[0])\n",
    "        \n",
    "        X_ = np.zeros((X.shape[0], num_clusters, X.shape[2]))\n",
    "        for k in range(num_clusters):\n",
    "            if len(neuron_clusters[k]) > 0:\n",
    "                X_[:, k, :] = np.mean(X[:, neuron_clusters[k], :], axis=1)\n",
    "\n",
    "        X_new.append(X_)\n",
    "    \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_spikes_thresholds = get_thresholds(avg_spikes, NEURON_CLUSTERS)\n",
    "X_train_new = cluster_neurons(X_train, avg_spikes_thresholds, NEURON_CLUSTERS)\n",
    "X_train_new = np.concatenate(X_train_new, axis=0)\n",
    "Y_train_new = np.concatenate(Y_train, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with Reduced Rank Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(session, X_train_session, Y_train_session, model, optimizer, loss_fn):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    X_train_session = torch.from_numpy(X_train_session).float()\n",
    "    Y_train_session = torch.from_numpy(Y_train_session).float()\n",
    "\n",
    "    Y_pred = model(session, X_train_session)\n",
    "    \n",
    "    loss = loss_fn(Y_pred, Y_train_session)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_rank_model = ReducedRankModel(\n",
    "    num_sessions,\n",
    "    [X_train[i].shape[1] for i in range(num_sessions)], \n",
    "    time_bins, \n",
    "    rank=RANK\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(reduced_rank_model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "loss = []\n",
    "\n",
    "for training_iteration in range(train_iters):\n",
    "    loss_iter = [session for session in range(num_sessions)]\n",
    "\n",
    "    for session in range(num_sessions):\n",
    "        loss_item = train_step(session, X_train[session], Y_train[session], reduced_rank_model, optimizer, loss_fn)\n",
    "        loss_iter[session] = loss_item\n",
    "        if training_iteration % 100 == 0:\n",
    "            print(f'Iteration {training_iteration}, Recording {session}, Loss {loss_item}')\n",
    "    \n",
    "    loss.append(loss_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve using plotly graphical objects\n",
    "loss = np.array(loss)\n",
    "\n",
    "fig = go.Figure()\n",
    "for session in range(num_sessions):\n",
    "    fig.add_trace(go.Scatter(y=loss[:, session], name=f'Recording {session}'))\n",
    "fig.update_layout(title=f'Loss Curve Rank {reduced_rank_model.rank}', xaxis_title='Iteration', yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recording(recording, X_test_recording, Y_test_recording, model, loss_fn, plot=False):\n",
    "    model.eval()\n",
    "    \n",
    "    X_test_recording = torch.from_numpy(X_test_recording).float()\n",
    "    Y_test_recording = torch.from_numpy(Y_test_recording).float()\n",
    "    Y_pred = model(recording, X_test_recording)\n",
    "    loss = loss_fn(Y_pred, Y_test_recording)\n",
    "\n",
    "    if plot:\n",
    "        for t in range(3):\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(y=Y_test_recording[t], name='Actual'))\n",
    "            fig.add_trace(go.Scatter(y=Y_pred[t].detach().numpy(), name='Predicted'))\n",
    "            fig.update_layout(title=f'Actual and Predicted Wheel Speeds, Recording {recording}, Trial {t}', xaxis_title='Time', yaxis_title='Value')\n",
    "            fig.show()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in range(num_sessions):\n",
    "    loss_item = evaluate_recording(recording, X_test[recording], Y_test[recording], reduced_rank_model, loss_fn, plot=True)\n",
    "    print(f'Evaluation, Recording {recording}, Loss {loss_item}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with Clustered Neurons by Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ridge regression on X_train_new and Y_train_new\n",
    "X_train_new = X_train_new.reshape(X_train_new.shape[0], -1)\n",
    "reg = Ridge(alpha=0.5)\n",
    "reg.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new = cluster_neurons(X_test, avg_spikes_thresholds, NEURON_CLUSTERS)\n",
    "X_test_new = np.concatenate(X_test_new, axis=0)\n",
    "X_test_new = X_test_new.reshape(X_test_new.shape[0], -1)\n",
    "Y_test_new = np.concatenate(Y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Ridge regression model\n",
    "Y_pred = reg.predict(X_test_new)\n",
    "loss = mean_squared_error(Y_test_new, Y_pred)\n",
    "print(f'Evaluation, Ridge Regression, Loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted wheel speeds against the actual wheel speeds\n",
    "for t in range(3):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=Y_test_new[t], name='Actual'))\n",
    "    fig.add_trace(go.Scatter(y=Y_pred[t], name='Predicted'))\n",
    "    fig.update_layout(title=f'Actual and Predicted Wheel Speeds, Ridge Regression, Trial {t}', xaxis_title='Time', yaxis_title='Value')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE for different number of neuron clusters\n",
    "mse_lst = []\n",
    "for i in range(10):\n",
    "    avg_spikes_thresholds = get_thresholds(avg_spikes, i+1)\n",
    "    X_train_new = cluster_neurons(X_train, avg_spikes_thresholds, i+1)\n",
    "    X_train_new = np.concatenate(X_train_new, axis=0)\n",
    "    Y_train_new = np.concatenate(Y_train, axis=0)\n",
    "\n",
    "    X_test_new = cluster_neurons(X_test, avg_spikes_thresholds, i+1)\n",
    "    X_test_new = np.concatenate(X_test_new, axis=0)\n",
    "    Y_test_new = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    X_train_new = X_train_new.reshape(X_train_new.shape[0], -1)\n",
    "    X_test_new = X_test_new.reshape(X_test_new.shape[0], -1)\n",
    "\n",
    "    reg = Ridge(alpha=0.5)\n",
    "    reg.fit(X_train_new, Y_train_new)\n",
    "\n",
    "    Y_pred = reg.predict(X_test_new)\n",
    "    mse = mean_squared_error(Y_test_new, Y_pred)\n",
    "    mse_lst.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mse against number of neuron clusters, highlighting bar height differences\n",
    "fig = go.Figure(data=[go.Bar(x=np.arange(len(mse_lst)), y=mse_lst)])\n",
    "fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)', marker_line_width=1.5,\n",
    "                  opacity=0.6)\n",
    "\n",
    "# set y-axis range\n",
    "low = min(mse_lst)\n",
    "high = max(mse_lst)\n",
    "fig.update_yaxes(range=[low - 0.0005, high + 0.0005])\n",
    "\n",
    "# relabel x-axis ticks\n",
    "fig.update_layout(xaxis_ticktext=np.arange(1, 11), xaxis_tickvals=np.arange(10))\n",
    "\n",
    "fig.update_layout(title='MSE vs Number of Neuron Clusters', xaxis_title='Number of Neuron Clusters', yaxis_title='MSE')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with ARD Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_spikes_thresholds = get_thresholds(avg_spikes, NEURON_CLUSTERS)\n",
    "X_train_new = cluster_neurons(X_train, avg_spikes_thresholds, NEURON_CLUSTERS)\n",
    "X_train_new = np.concatenate(X_train_new, axis=0)\n",
    "Y_train_new = np.concatenate(Y_train, axis=0)\n",
    "\n",
    "X_test_new = cluster_neurons(X_test, avg_spikes_thresholds, NEURON_CLUSTERS)\n",
    "X_test_new = np.concatenate(X_test_new, axis=0)\n",
    "Y_test_new = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "X_train_new = X_train_new.reshape(X_train_new.shape[0], -1)\n",
    "X_test_new = X_test_new.reshape(X_test_new.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ARD regression with RegressorChain on X_train_new and Y_train_new\n",
    "reg = RegressorChain(ARDRegression(alpha_1=1e-4, alpha_2=1e-4, lambda_1=1, lambda_2=0.5))\n",
    "reg.fit(X_train_new, Y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ARD regression model\n",
    "Y_pred = reg.predict(X_test_new)\n",
    "loss = mean_squared_error(Y_test_new, Y_pred)\n",
    "print(f'Evaluation, ARD Regression, Loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted wheel speeds against the actual wheel speeds\n",
    "for t in range(3):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=Y_test_new[t], name='Actual'))\n",
    "    fig.add_trace(go.Scatter(y=Y_pred[t], name='Predicted'))\n",
    "    fig.update_layout(title=f'Actual and Predicted Wheel Speeds, ARD Regression, Trial {t}', xaxis_title='Time', yaxis_title='Value')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE for different number of neuron clusters\n",
    "mse_lst = []\n",
    "\n",
    "for i in range(10):\n",
    "    avg_spikes_thresholds = get_thresholds(avg_spikes, i+1)\n",
    "    X_train_new = cluster_neurons(X_train, avg_spikes_thresholds, i+1)\n",
    "    X_train_new = np.concatenate(X_train_new, axis=0)\n",
    "    Y_train_new = np.concatenate(Y_train, axis=0)\n",
    "\n",
    "    X_test_new = cluster_neurons(X_test, avg_spikes_thresholds, i+1)\n",
    "    X_test_new = np.concatenate(X_test_new, axis=0)\n",
    "    Y_test_new = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    X_train_new = X_train_new.reshape(X_train_new.shape[0], -1)\n",
    "    X_test_new = X_test_new.reshape(X_test_new.shape[0], -1)\n",
    "\n",
    "    reg = RegressorChain(ARDRegression())\n",
    "    reg.fit(X_train_new, Y_train_new)\n",
    "\n",
    "    Y_pred = reg.predict(X_test_new)\n",
    "    mse = mean_squared_error(Y_test_new, Y_pred)\n",
    "    mse_lst.append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mse against number of neuron clusters, highlighting bar height differences\n",
    "fig = go.Figure(data=[go.Bar(x=np.arange(len(mse_lst)), y=mse_lst)])\n",
    "fig.update_traces(marker_color='rgb(158,202,225)', marker_line_color='rgb(8,48,107)', marker_line_width=1.5,\n",
    "                  opacity=0.6)\n",
    "\n",
    "# set y-axis range\n",
    "low = min(mse_lst)\n",
    "high = max(mse_lst)\n",
    "fig.update_yaxes(range=[low - 0.0005, high + 0.0005])\n",
    "\n",
    "# relabel x-axis ticks\n",
    "fig.update_layout(xaxis_ticktext=np.arange(1, 11), xaxis_tickvals=np.arange(10))\n",
    "\n",
    "fig.update_layout(title='MSE of ARD Regression Model', xaxis_title='Number of Neuron Clusters', yaxis_title='MSE')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding with Autoregressive Model of Order 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding with Poisson regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_and_max_matrices(X):\n",
    "    avg_matrix = np.empty((0, time_bins))\n",
    "    max_matrix = np.empty((0, time_bins))\n",
    "    \n",
    "    for session in X:\n",
    "        avg_spikes = np.mean(session, axis=1)\n",
    "        max_spikes = np.max(session, axis=1)\n",
    "        avg_matrix = np.vstack((avg_matrix, avg_spikes))\n",
    "        max_matrix = np.vstack((max_matrix, max_spikes))\n",
    "        \n",
    "    return avg_matrix, max_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_spikes, train_max_spikes = create_avg_and_max_matrices(X_train)\n",
    "test_avg_spikes, test_max_spikes = create_avg_and_max_matrices(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wheel_speed_matrix(Y):\n",
    "    wheel_speed_matrix = np.empty((0, time_bins))\n",
    "\n",
    "    for session in Y:\n",
    "        wheel_speed_matrix = np.vstack((wheel_speed_matrix, session))\n",
    "    \n",
    "    return wheel_speed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wheel_speeds = create_wheel_speed_matrix(Y_train)\n",
    "test_wheel_speeds = create_wheel_speed_matrix(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_spikes, test_spikes, train_speeds, test_speeds):\n",
    "    encoders = [linear_model.PoissonRegressor() for _ in range(time_bins)]\n",
    "    spikes_pred = np.zeros(test_spikes.shape)\n",
    "\n",
    "    for t in range(time_bins):\n",
    "        train_speeds_dim = np.expand_dims(train_speeds[:,t], axis=1)\n",
    "        test_speeds_dim = np.expand_dims(test_speeds[:,t], axis=1)\n",
    "        encoders[t].fit(train_speeds_dim, train_spikes[:,t])\n",
    "        spikes_pred[:,t] = encoders[t].predict(test_speeds_dim)\n",
    "        \n",
    "    return encoders, spikes_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_encoders, avg_spikes_pred = train_and_predict(train_avg_spikes, test_avg_spikes, train_wheel_speeds, test_wheel_speeds) \n",
    "max_encoders, max_spikes_pred = train_and_predict(train_max_spikes, test_max_spikes, train_wheel_speeds, test_wheel_speeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MSE for average spikes: {mean_squared_error(test_avg_spikes, avg_spikes_pred)}')\n",
    "print(f'MSE for maximum spikes: {mean_squared_error(test_max_spikes, max_spikes_pred)}')\n",
    "print(f'R-squared for average spikes: {r2_score(test_avg_spikes, avg_spikes_pred)}')\n",
    "print(f'R-squared for maximum spikes: {r2_score(test_max_spikes, max_spikes_pred)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
